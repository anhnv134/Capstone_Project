# -*- coding: utf-8 -*-
"""Colab_7884_CapstoneShare_V1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HhNeMnG1Lzth55_pFInPcMX_qjhYf2wJ
"""

"""##**Algorithm 1: Data Preprocessing and Selection of Traded Instruments**

<div style="text-align: justiry;">

**Goal:** Our goal here is to identify which NSE stocks, futures to apply the breakout strategy to. We will focus on instruments with sufficient 1)liquidity 2)volatility and 3)trendliness.

</div>

**Inputs:**
- Universe of NSE instruments
- Intraday OHLCV data (O: open, H: high, L: low, C: close, V: volume) for each instrument
- Chosen intraday time frame (e.g., 5-min bars)
- Minimum liquidity threshold (e.g., average volume over X days)
- Minimum volatility threshold (e.g., ATR > certain value)
- Optional trend filter (e.g., ADX > Y)
"""

#mount google drive
#from google.colab import drive
#drive.mount('/content/drive')

#Acceptance Rates
#import necessary libraries
import os
import glob
import shutil
import pandas as pd
import numpy as np
from collections import defaultdict
from itertools import product
import logging
from datetime import datetime
import warnings
warnings.filterwarnings("ignore")

LOG_DIR = "C:/Users/vuanh/Downloads/colab"

# Log file path
LOG_FILE = os.path.join(LOG_DIR, "Algo1_logfile.txt")

# ---------------------------
# LOG FUNCTION DEFINITIONS
# ---------------------------

def log_message(message, log_file=LOG_FILE):
    """
    Logs a message with a timestamp to the specified log file and prints it.

    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    full_message = f"[{timestamp}] {message}"
    print(full_message)
    try:
        with open(log_file, 'a') as f:
            f.write(full_message + "\n")
    except FileNotFoundError:
        print(f"ERROR: Log file directory does not exist: {os.path.dirname(log_file)}")
    except Exception as e:
        print(f"ERROR: Failed to write to log file. Exception: {e}")

########################################
# GLOBAL SETTINGS & PATHS
########################################

BASE_2021_DIR = "C:/Users/vuanh/Downloads/2021"

# Here we define these global defaults
ATR_PERIOD = 14   # used in calculate_atr
ADX_PERIOD = 14   # used in calculate_adx


########################################
#  STEP A: COMBINE 12 MONTHS
########################################

def load_and_clean_csv(csv_path: str) -> pd.DataFrame or None:
    """
    Loads one CSV of intraday data for one month and one ticker.
    Expects columns:
      <date>, <time>, <open>, <high>, <low>, <close>, <volume> (optionally <ticker>)
    Merges <data> + <time> => DateTime index, drops incomplete rows.
    """
    if not os.path.exists(csv_path):
        log_message(f"CSV path does not exist: {csv_path}")
        return None

    try:
      df = pd.read_csv(csv_path)
    except Exception as e:
      log_message(f"Error reading CSV: {csv_path} - {e}")
      return None

    needed_cols = {'<date>', '<time>', '<open>', '<high>', '<low>', '<close>', '<volume>'}
    if not needed_cols.issubset(df.columns):
        log_message(f"CSV '{csv_path}' is missing required columns.")
        return None

    # Combine <date> + <time> => DateTime index
    df['DateTime'] = pd.to_datetime(df['<date>'] + " " + df['<time>'], errors='coerce')
    df.set_index('DateTime', inplace=True)
    df.drop(['<date>', '<time>'], axis=1, inplace=True)

    # Convert numeric columns
    numeric_cols = ['<open>', '<high>', '<low>', '<close>', '<volume>']
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')


    #rename columns
    rename_map = {
        '<open>': 'Open',
        '<high>': 'High',
        '<low>':  'Low',
        '<close>':'Close',
        '<volume>':'Volume',
        '<ticker>':'Ticker'
    }
    df.rename(columns=rename_map, inplace=True)

    df.sort_index(inplace=True)
    df.dropna(subset=['Open','High','Low','Close','Volume'], how='any', inplace=True)

    if len(df) == 0:
        return None
    return df


def combine_annual_data(base_dir: str) -> dict:
    """
    - Finds monthly subfolders in base_dir
    - Gathers CSV paths for each ticker across all months
    - Loads each CSV, merges into a single yearly DataFrame per ticker
    Returns {ticker: df_combined}
    """
    # Check if base_dir exists
    if not os.path.isdir(base_dir):
        log_message(f"Base directory '{base_dir}' does not exist.")
        return {}
    # Identify monthly folders
    month_folders = []
    for item in sorted(os.listdir(base_dir)):
        subp = os.path.join(base_dir, item)
        if os.path.isdir(subp):
            month_folders.append(subp)
    log_message(f"Found {len(month_folders)} monthly folders under '{base_dir}':")

    #Build ticker->list_of_csv
    ticker_to_csvs = defaultdict(list)
    for mfolder in month_folders:
        all_csvs = glob.glob(os.path.join(mfolder, "*.csv"))
        log_message(f"{os.path.basename(mfolder)} => {len(all_csvs)} CSV files found.")
        for cpath in all_csvs:


            ticker_name = os.path.splitext(os.path.basename(cpath))[0]
            ticker_to_csvs[ticker_name].append(cpath)

    combined_results = {}
    for ticker, csv_paths in ticker_to_csvs.items():
        df_list = []
        for cpath in csv_paths:
            df_part = load_and_clean_csv(cpath)
            if df_part is not None and len(df_part) > 0:
                df_list.append(df_part)
        if not df_list:
            log_message(f"No valid data for ticker '{ticker}'. Skipping.")
            continue
        df_combined = pd.concat(df_list, axis=0).sort_index()
        # drop duplicate timestamps
        df_combined = df_combined[~df_combined.index.duplicated(keep='first')]
        combined_results[ticker] = df_combined
    log_message(f"Total distinct tickers across subfolders: {len(ticker_to_csvs)}")
    log_message(f" Tickers with valid data after merging: {len(combined_results)}")
    return combined_results


########################################
#  STEP B: RESAMPLE TO 5MIN
########################################
def resample_to_5min(df_irregular: pd.DataFrame) -> pd.DataFrame:
    """
    Resample one ticker's data to uniform 5-minute bars.
    If no trades in a bar, result might have NaNs => we drop them.
    """

    df_5m = df_irregular.resample('5min').agg({
        'Open': 'first',
        'High' : 'max',
        'Low'   : 'min',
        'Close' : 'last',
        'Volume': 'sum'
    })
    # drop intervals where OHLC is all NaN
    df_5m.dropna(subset=['Open','High','Low','Close'], how='any', inplace=True)
    return df_5m



def resample_all_to_5min(combined_results: dict) -> dict:
    """
    For each ticker's full-year DataFrame in combined_results,
    produce a 5-min resampled DataFrame.
    Return dict {ticker: df_5m}
    """

    resampled_dict = {}
    for ticker, df_full in combined_results.items():
        if df_full is None or len(df_full) == 0:
            log_message(f"Skipping empty DataFrame for ticker: {ticker}")
            continue
        df_5m = resample_to_5min(df_full)
        if len(df_5m) > 0:
            resampled_dict[ticker] = df_5m
    return resampled_dict

########################################
#  STEP C: ALGORITHM 1 INDICATORS
########################################
def calculate_atr(df: pd.DataFrame, period: int) -> pd.Series:
    """
    Calculate the Average True Range (ATR) with Wilder's smoothing.
    :param df: DataFrame with 'High', 'Low', and 'Close' columns.
    :param period: Lookback period for ATR calculation.
    :return: ATR as a Pandas Series.
    """
    # Calculate True Range (TR)
    hl = df['High'] - df['Low']
    hc = (df['High'] - df['Close'].shift(1)).abs()
    lc = (df['Low'] - df['Close'].shift(1)).abs()
    tr = np.maximum.reduce([hl, hc, lc])

    #Convert TR to Pandas Series before applying Wilder's smoothing
    tr_series = pd.Series(tr, index=df.index)

    # Apply Wilder's smoothing to TR
    atr = tr_series.ewm(alpha=1/period, adjust=False).mean()

    return atr

def calculate_adx(df: pd.DataFrame, period: int) -> pd.Series:

    """
    Calculate the Average Directional Index (ADX).with Wilder's smoothing.
    :param df: DataFrame with 'High', 'Low', and 'Close' columns.
    :param period: Lookback period for ADX calculation.
    :return: ADX as a Pandas Series.
    """
    # Calcualte directional movements
    up_move = df['High'].diff()
    down_move = -df['Low'].diff()

    plus_dm = np.where((up_move > 0) & (up_move > down_move), up_move, 0)
    minus_dm = np.where((down_move > 0) & (down_move > up_move), down_move, 0)

    # Calculate True Range (TR)
    high_low = df['High'] - df['Low']
    high_close = np.abs(df['High'] - df['Close'].shift(1))
    low_close = np.abs(df['Low'] - df['Close'].shift(1))
    tr = np.maximum.reduce([high_low, high_close, low_close])

    # convert numpy array to Pandas Series
    tr_series = pd.Series(tr, index=df.index)
    plus_dm_series = pd.Series(plus_dm, index=df.index)
    minus_dm_series = pd.Series(minus_dm, index=df.index)

    # Smooth TR, +DM, and -DM using Wilder's smoothing
    tr_smoothed = tr_series.ewm(alpha=1/period, adjust=False).mean()
    plus_dm_smoothed = plus_dm_series.ewm(alpha=1/period, adjust=False).mean()
    minus_dm_smoothed = minus_dm_series.ewm(alpha=1/period, adjust=False).mean()

    # Calculate +DI and -DI
    plus_di = (plus_dm_smoothed / tr_smoothed) * 100
    minus_di = (minus_dm_smoothed / tr_smoothed) * 100

    # Calculate Directional Index (DX)
    dx = (np.abs(plus_di - minus_di) / (plus_di + minus_di)) * 100

    # Calculate ADX
    adx = dx.ewm(alpha=1/period, adjust=False).mean()

    return pd.Series(adx, index=df.index)

def passes_algorithm1(df_5m: pd.DataFrame,
                      roll_vol_bars:int,
                      min_liquidity:float,
                      min_volatility:float,
                      trendiness_thr:float) -> bool:
    """
    Modified version of function that takes threshold parameters.
    1) Rolling-average Volume check
    2) ATR check
    3) ADX check
    """

    # 1) rolling-average volume
    roll_vol = df_5m['Volume'].rolling(roll_vol_bars).mean()
    if roll_vol.dropna().empty:
        return False
    last_avg_vol = roll_vol.iloc[-1]
    if last_avg_vol < min_liquidity:
        return False
    # 2) ATR check

    atr_ser = calculate_atr(df_5m, ATR_PERIOD)
    if atr_ser.dropna().empty:
        return False
    last_atr = atr_ser.iloc[-1]
    if pd.isna(last_atr) or last_atr < min_volatility:
        return False

    # 3) ADX check
    adx_ser = calculate_adx(df_5m, ADX_PERIOD)
    if adx_ser.dropna().empty:
        return False
    last_adx = adx_ser.iloc[-1]
    if pd.isna(last_adx) or last_adx < trendiness_thr:
        return False

    return True


def filter_instruments_by_algorithm1(
    resampled_dict: dict,
    roll_vol_bars: int,
    min_liquidity: float,
    min_volatility: float,
    trendiness_thr: float
) -> list:
    """
    Applies `passes_algorithm1` to each ticker in `resampled_dict`.
    Returns a list of ticker names that pass all thresholds.
    """

    chosen_instruments = []
    for tkr, df_5m in resampled_dict.items():
        if passes_algorithm1(df_5m, roll_vol_bars, min_liquidity, min_volatility, trendiness_thr):
            chosen_instruments.append(tkr)
    return chosen_instruments


# ============== KEY FUNCTION FOR PARAM SEARCH ==============

def run_algorithm1_pipeline(
    resampled_dict: dict,
    min_liquidity: float,
    min_volatility: float,
    trendiness_thr: float,
    roll_vol_bars: int = 100
) -> list:

    # C) Filter
    final_instruments = filter_instruments_by_algorithm1(
        resampled_dict,
        roll_vol_bars,
        min_liquidity,
        min_volatility,
        trendiness_thr
    )
    return final_instruments


def main():
    """
    Define a small grid of parmeter combos,
    run the pipeline for each, and store the results.
    """
    param_grid = {
      'min_liquidity':    [20000, 50000, 100000],
      'min_volatility':   [0.3, 0.5, 0.7, 1.0],
      'trendiness_thr':   [25,30],
      'roll_vol_bars':    [100]
    }


    results = []

    """
    Runs the entire pipeline with the given threshold parameters:
      1) Combine monthly => yearly
      2) Resample to 5-min
      3) Filter by Algorithm 1
    Returns the list of passing tickers.
    """

    # A) Combine monthly => yearly
    combined_results = combine_annual_data(BASE_2021_DIR)
    if not combined_results:
      log_message("No combined data to process.")
      return []
    # B) Resample to 5-min
    resampled_dict = resample_all_to_5min(combined_results)

    # create all combos
    for ml in param_grid['min_liquidity']:
        for mv in param_grid['min_volatility']:
            for adx_thr in param_grid['trendiness_thr']:
                for rv_bars in param_grid['roll_vol_bars']:
                    # Run pipeline
                    passing_tickers = run_algorithm1_pipeline(
                        resampled_dict,
                        min_liquidity=ml,
                        min_volatility=mv,
                        trendiness_thr=adx_thr,
                        roll_vol_bars=rv_bars
                    )
                    n_pass = len(passing_tickers)
                    # Record
                    results.append({
                        'min_liquidity': ml,
                        'min_volatility_thr'  : mv,
                        'trendiness_thr': adx_thr,
                        'roll_vol_bars' : rv_bars,
                        'num_pass' : n_pass,
                        'tickers'  : passing_tickers
                    })

    # DataFrame of results
    results_df = pd.DataFrame(results)
    results_csv_path = ("C:/Users/vuanh/Downloads/colab/algo1_threshold_search_results.csv")

    try:
      results_df.to_csv(results_csv_path, index=False)
      log_message(f"Results saved to '{results_csv_path}'.")
    except Exception as e:
      log_message(f"Error saving results to '{results_csv_path}': {e}")
    log_message("\n=== THRESHOLD SEARCH COMPLETE ===")
    log_message(f"\n{results_df[['min_liquidity','min_volatility_thr','trendiness_thr','roll_vol_bars','num_pass']]}")
    log_message(f"\nFull detaisl with tickers in '{results_csv_path}'.")

if __name__ == "__main__":
    main()
